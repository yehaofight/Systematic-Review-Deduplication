{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c59c80cc-26d7-4116-9200-1365ca393f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功！您的 EndNote 資料已經在 Python 裡了：\n",
      "   Conference Proceedings     Unnamed: 1  2014  \\\n",
      "0  Conference Proceedings            NaN  2018   \n",
      "1         Journal Article            NaN  2021   \n",
      "2  Conference Proceedings            NaN  2024   \n",
      "3  Conference Proceedings            NaN  2024   \n",
      "4  Conference Proceedings  蔡孟臻; 嚴一凡; 吳東昇  2024   \n",
      "\n",
      "      32nd AIAA Applied Aerodynamics Conference 2014 Unnamed: 4  \\\n",
      "0                       Nonlinear Photonics, NP 2018        NaN   \n",
      "1  Erratum: Neutron Spin Echo Spectroscopy as a U...        NaN   \n",
      "2  11th International Conference on E-Health and ...        NaN   \n",
      "3  2024 International Symposium on 3D Analysis of...        NaN   \n",
      "4                【論文摘要】探討使用嚴氏撥筋術改善因筋膜沾黏因素所引起之痠痛的臨床效益        NaN   \n",
      "\n",
      "          Unnamed: 5 Unnamed: 6 Unnamed: 7         Unnamed: 8  Unnamed: 9  \\\n",
      "0                NaN        NaN        NaN  Part F108-NP 2018         NaN   \n",
      "1          J Vis Exp        NaN        NaN                NaN         NaN   \n",
      "2  IFMBE Proceedings        NaN        NaN                109         NaN   \n",
      "3                NaN        NaN        NaN                NaN         NaN   \n",
      "4                NaN        NaN  臺灣輔助醫學醫學會                NaN         NaN   \n",
      "\n",
      "   ... Unnamed: 42 Unnamed: 43 Unnamed: 44 Unnamed: 45 Unnamed: 46  \\\n",
      "0  ...         NaN         NaN         NaN         NaN         NaN   \n",
      "1  ...         NaN         NaN         NaN         NaN         NaN   \n",
      "2  ...         NaN         NaN         NaN         NaN         NaN   \n",
      "3  ...         NaN         NaN         NaN         NaN         NaN   \n",
      "4  ...         NaN         NaN         NaN         NaN         NaN   \n",
      "\n",
      "  Unnamed: 47 Unnamed: 48         Scopus Unnamed: 50 Unnamed: 51  \n",
      "0         NaN         NaN         Scopus         NaN         NaN  \n",
      "1         NaN         NaN            NaN         NLM         eng  \n",
      "2         NaN         NaN         Scopus         NaN         NaN  \n",
      "3         NaN         NaN         Scopus         NaN         NaN  \n",
      "4         NaN         NaN  AiritiLibrary      Airiti        繁體中文  \n",
      "\n",
      "[5 rows x 52 columns]\n",
      "\n",
      "總共讀取了 2731 筆文獻。\n"
     ]
    }
   ],
   "source": [
    "# 1. 載入我們需要的工具 (Pandas)\n",
    "import pandas as pd\n",
    "\n",
    "# 2. 您的檔案名稱\n",
    "file_path = '系統性文獻回顧.txt'\n",
    "\n",
    "# 3. 讀取您的檔案 (sep='\\t' 就是告訴它這是 Tab 分隔檔)\n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "# 4. 顯示您文獻資料表的前 5 行，確認讀取成功\n",
    "print(\"成功！您的 EndNote 資料已經在 Python 裡了：\")\n",
    "print(df.head())\n",
    "\n",
    "# 5. 顯示總共有多少筆文獻 (方便我們下一步抓重複)\n",
    "print(f\"\\n總共讀取了 {len(df)} 筆文獻。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae5451db-4bef-4aa0-9114-5075ecdd0357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功！這是『修正後』的表格：\n",
      "                       0    1     2   \\\n",
      "0  Conference Proceedings  NaN  2014   \n",
      "1  Conference Proceedings  NaN  2018   \n",
      "2         Journal Article  NaN  2021   \n",
      "3  Conference Proceedings  NaN  2024   \n",
      "4  Conference Proceedings  NaN  2024   \n",
      "\n",
      "                                                  3    4                  5   \\\n",
      "0     32nd AIAA Applied Aerodynamics Conference 2014  NaN                NaN   \n",
      "1                       Nonlinear Photonics, NP 2018  NaN                NaN   \n",
      "2  Erratum: Neutron Spin Echo Spectroscopy as a U...  NaN          J Vis Exp   \n",
      "3  11th International Conference on E-Health and ...  NaN  IFMBE Proceedings   \n",
      "4  2024 International Symposium on 3D Analysis of...  NaN                NaN   \n",
      "\n",
      "    6    7                  8   9   ...  42   43  44  45  46  47   48      49  \\\n",
      "0  NaN  NaN                NaN NaN  ... NaN  NaN NaN NaN NaN NaN  NaN  Scopus   \n",
      "1  NaN  NaN  Part F108-NP 2018 NaN  ... NaN  NaN NaN NaN NaN NaN  NaN  Scopus   \n",
      "2  NaN  NaN                NaN NaN  ... NaN  NaN NaN NaN NaN NaN  NaN     NaN   \n",
      "3  NaN  NaN                109 NaN  ... NaN  NaN NaN NaN NaN NaN  NaN  Scopus   \n",
      "4  NaN  NaN                NaN NaN  ... NaN  NaN NaN NaN NaN NaN  NaN  Scopus   \n",
      "\n",
      "    50   51  \n",
      "0  NaN  NaN  \n",
      "1  NaN  NaN  \n",
      "2  NLM  eng  \n",
      "3  NaN  NaN  \n",
      "4  NaN  NaN  \n",
      "\n",
      "[5 rows x 52 columns]\n",
      "\n",
      "總共讀取了 2732 筆文獻。\n"
     ]
    }
   ],
   "source": [
    "# 1. 載入我們需要的工具 (Pandas)\n",
    "import pandas as pd\n",
    "\n",
    "# 2. 您的檔案名稱\n",
    "file_path = '系統性文獻回顧.txt'\n",
    "\n",
    "# 3. 【關鍵修正】\n",
    "# 我們加入了 header=None，告訴 Pandas \"這個檔案沒有標題列\"\n",
    "# 這樣它就會用 0, 1, 2, 3... 作為欄位名稱，資料就不會錯亂\n",
    "df = pd.read_csv(file_path, sep='\\t', header=None)\n",
    "\n",
    "# 4. 顯示您文獻資料表的前 5 行 (這次會很整齊)\n",
    "print(\"成功！這是『修正後』的表格：\")\n",
    "print(df.head())\n",
    "\n",
    "# 5. 顯示總共有多少筆文獻\n",
    "print(f\"\\n總共讀取了 {len(df)} 筆文獻。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91cf9f64-3836-4ff0-a1ea-b330b846b954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 這是我們挑選出的『年份』和『標題』---\n",
      "   Year                                              Title\n",
      "0  2014     32nd AIAA Applied Aerodynamics Conference 2014\n",
      "1  2018                       Nonlinear Photonics, NP 2018\n",
      "2  2021  Erratum: Neutron Spin Echo Spectroscopy as a U...\n",
      "3  2024  11th International Conference on E-Health and ...\n",
      "4  2024  2024 International Symposium on 3D Analysis of...\n",
      "\n",
      "--- 標題欄位的空值檢查 ---\n",
      "標題 (Title) 欄位中，總共有 177 筆是空白的。\n"
     ]
    }
   ],
   "source": [
    "# 1. 建立一個新的、乾淨的表格 (DataFrame)\n",
    "# 我們只挑出「欄位 2」和「欄位 3」\n",
    "# 並幫它們重新命名 (rename)\n",
    "clean_df = df[[2, 3]].rename(columns={\n",
    "    2: 'Year',\n",
    "    3: 'Title'\n",
    "})\n",
    "\n",
    "# 2. 顯示這個「乾淨表格」的前 5 行\n",
    "print(\"--- 這是我們挑選出的『年份』和『標題』---\")\n",
    "print(clean_df.head())\n",
    "\n",
    "# 3. 檢查一下「標題」欄位有沒有「空值」(NaN)\n",
    "# 這對下一步抓重複很重要\n",
    "print(\"\\n--- 標題欄位的空值檢查 ---\")\n",
    "print(f\"標題 (Title) 欄位中，總共有 {clean_df['Title'].isna().sum()} 筆是空白的。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "861b0b45-ce6d-4539-a2e3-59f0a53d4e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 以下是 Python 認為『標題為空值』的 177 筆資料 (最多顯示 10 筆) ---\n",
      "                                                                                                                                                                                                                                                                                                                                      0    1                                  2    3                                                                                                                                                                                                                                                                                                                                                                                          4    5    6    7    8   9    10   11       12   13   14   15   16   17   18   19   20   21   22   23  24  25   26   27  28  29  30   31   32  33   34  35  36   37   38   39  40   41  42   43  44  45  46  47   48   49   50   51\n",
      "154  Funding This work was partially supported by the Suzhou Municipal Science and Technology Project (Grant No. SKY2022011), the Elderly Health Research Project of Jiangsu Province (Grant No. LKZ2022009), and the Science and Technology Project for \"Star of Medical Imaging\" of Suzhou Medical Association (Grant No. 2022YX-M03).  NaN                                NaN  NaN                                                                                                                                                                                                                                                                                                                                                                                        NaN  NaN  NaN  NaN  NaN NaN  NaN  NaN      NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN NaN NaN  NaN  NaN NaN NaN NaN  NaN  NaN NaN  NaN NaN NaN  NaN  NaN  NaN NaN  NaN NaN  NaN NaN NaN NaN NaN  NaN  NaN  NaN  NaN\n",
      "155                                                                                                                                                                                                                                                                                                                                    3  NaN                                NaN  NaN                                                                                                                                                                                                                                                                                                                                                                                        NaN  NaN  NaN  NaN  NaN NaN  NaN  NaN      NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN NaN NaN  NaN  NaN NaN NaN NaN  NaN  NaN NaN  NaN NaN NaN  NaN  NaN  NaN NaN  NaN NaN  NaN NaN NaN NaN NaN  NaN  NaN  NaN  NaN\n",
      "156                                                                                                                                                                                                                                                                                                                                    1  NaN                                NaN  NaN                                                                                                                                                                                                                                                                                                                                                                                        NaN  NaN  NaN  NaN  NaN NaN  NaN  NaN      NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN NaN NaN  NaN  NaN NaN NaN NaN  NaN  NaN NaN  NaN NaN NaN  NaN  NaN  NaN NaN  NaN NaN  NaN NaN NaN NaN NaN  NaN  NaN  NaN  NaN\n",
      "157                                                                                                                                                                                                                                                                                                                                    5  NaN                                NaN  NaN                                                                                                                                                                                                                                                                                                                                                                                        NaN  NaN  NaN  NaN  NaN NaN  NaN  NaN      NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN NaN NaN  NaN  NaN NaN NaN NaN  NaN  NaN NaN  NaN NaN NaN  NaN  NaN  NaN NaN  NaN NaN  NaN NaN NaN NaN NaN  NaN  NaN  NaN  NaN\n",
      "158                                                                                                                                                                                                                                                                                                                   Frontiers media sa  NaN                                NaN  NaN                                                                                                                                                                                                                                                                                                                                                                                        NaN  NaN  NaN  NaN  NaN NaN  NaN  NaN      NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN NaN NaN  NaN  NaN NaN NaN NaN  NaN  NaN NaN  NaN NaN NaN  NaN  NaN  NaN NaN  NaN NaN  NaN NaN NaN NaN NaN  NaN  NaN  NaN  NaN\n",
      "159                                                                                                                                                                                                                                                                                                                            Lausanne\"  NaN  <Go to ISI>://WOS:001033414300001  NaN  [Bao, Yiqing; Ya, Yang; Zhang, Chenchen; Wang, Erlei; Fan, Guohua] Soochow Univ, Affiliated Hosp 2, Dept Radiol, Suzhou, Peoples R China; [Liu, Jing] Soochow Univ, Affiliated Hosp 2, Dept Neurol, Suzhou, Peoples R China\\r\\nWang, ER; Fan, GH (Corresponding author), Soochow Univ, Affiliated Hosp 2, Dept Radiol, Suzhou, Peoples R China\\r\\nfangh22@126.com; erlei_wang_jyj@163.com  NaN  NaN  NaN  NaN NaN  NaN  NaN  English  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN NaN NaN  NaN  NaN NaN NaN NaN  NaN  NaN NaN  NaN NaN NaN  NaN  NaN  NaN NaN  NaN NaN  NaN NaN NaN NaN NaN  NaN  NaN  NaN  NaN\n",
      "187                                                                                                                                                                                                                                                                 IRCCS Santa Lucia Foundation, Via Ardeatina 306, 00179 Roma, Italy.\"  NaN                                NaN  NaN                                                                                                                                                                                                                                                                                                                                                                                        NaN  NaN  NaN  NLM  eng NaN  NaN  NaN      NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN NaN NaN  NaN  NaN NaN NaN NaN  NaN  NaN NaN  NaN NaN NaN  NaN  NaN  NaN NaN  NaN NaN  NaN NaN NaN NaN NaN  NaN  NaN  NaN  NaN\n",
      "189                                                                                                                                                                                                                                                                 IRCSS Fondazione Santa Lucia, Via Ardeatina 306, 00179 Roma, Italy.\"  NaN                                NaN  NaN                                                                                                                                                                                                                                                                                                                                                                                        NaN  NaN  NaN  NLM  eng NaN  NaN  NaN      NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN NaN NaN  NaN  NaN NaN NaN NaN  NaN  NaN NaN  NaN NaN NaN  NaN  NaN  NaN NaN  NaN NaN  NaN NaN NaN NaN NaN  NaN  NaN  NaN  NaN\n",
      "197                                                                                                                                                                    Clinical Laboratory of Experimental Neurorehabilitation, Fondazione Santa Lucia (Scientific Institute for Research Hospitalization and Health Care), Rome, Italy.  NaN                                NaN  NaN                                                                                                                                                                                                                                                                                                                                                                                        NaN  NaN  NaN  NaN  NaN NaN  NaN  NaN      NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN NaN NaN  NaN  NaN NaN NaN NaN  NaN  NaN NaN  NaN NaN NaN  NaN  NaN  NaN NaN  NaN NaN  NaN NaN NaN NaN NaN  NaN  NaN  NaN  NaN\n",
      "198                                                                                                                                       Interuniversity Centre of Bioengineering of the Human Neuromusculoskeletal System (BOHNES), Department of Movement, Human and Health Sciences, University of Rome \"Foro Italico\", Rome, Italy.  NaN                                NaN  NaN                                                                                                                                                                                                                                                                                                                                                                                        NaN  NaN  NaN  NaN  NaN NaN  NaN  NaN      NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN NaN NaN  NaN  NaN NaN NaN NaN  NaN  NaN NaN  NaN NaN NaN  NaN  NaN  NaN NaN  NaN NaN  NaN NaN NaN NaN NaN  NaN  NaN  NaN  NaN\n",
      "\n",
      "總共找到 177 筆有問題的資料。\n"
     ]
    }
   ],
   "source": [
    "# 【偵錯程式碼】\n",
    "\n",
    "# 1. 抓出有問題的資料\n",
    "# 我們使用最一開始的那個 df (有 52 欄的那個)\n",
    "# df[3].isna() 會幫我們找到所有「第 3 欄是空白」的資料\n",
    "problem_rows = df[df[3].isna()]\n",
    "\n",
    "print(f\"--- 以下是 Python 認為『標題為空值』的 177 筆資料 (最多顯示 10 筆) ---\")\n",
    "\n",
    "# 2. 顯示這些資料\n",
    "# .head(10) 顯示前 10 筆\n",
    "# .to_string() 確保 Jupyter 會把所有 52 欄都顯示出來，而不是用 ... 藏起來\n",
    "print(problem_rows.head(10).to_string())\n",
    "\n",
    "print(f\"\\n總共找到 {len(problem_rows)} 筆有問題的資料。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b87f12d-353e-4429-b346-37c4e86d9c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fuzzywuzzy\n",
      "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting python-Levenshtein\n",
      "  Downloading python_levenshtein-0.27.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting Levenshtein==0.27.3 (from python-Levenshtein)\n",
      "  Downloading levenshtein-0.27.3-cp312-cp312-win_amd64.whl.metadata (3.7 kB)\n",
      "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.27.3->python-Levenshtein)\n",
      "  Downloading rapidfuzz-3.14.3-cp312-cp312-win_amd64.whl.metadata (12 kB)\n",
      "Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
      "Downloading python_levenshtein-0.27.3-py3-none-any.whl (9.5 kB)\n",
      "Downloading levenshtein-0.27.3-cp312-cp312-win_amd64.whl (94 kB)\n",
      "   ---------------------------------------- 0.0/94.9 kB ? eta -:--:--\n",
      "   ------------------------- -------------- 61.4/94.9 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 94.9/94.9 kB 1.4 MB/s eta 0:00:00\n",
      "Downloading rapidfuzz-3.14.3-cp312-cp312-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.2/1.5 MB 5.3 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.4/1.5 MB 5.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.0/1.5 MB 8.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 10.8 MB/s eta 0:00:00\n",
      "Installing collected packages: fuzzywuzzy, rapidfuzz, Levenshtein, python-Levenshtein\n",
      "Successfully installed Levenshtein-0.27.3 fuzzywuzzy-0.18.0 python-Levenshtein-0.27.3 rapidfuzz-3.14.3\n"
     ]
    }
   ],
   "source": [
    "# 【注意：前面有一個「!」驚嘆號】\n",
    "# 這是在告訴 Jupyter，這不是 Python 程式碼，而是一個「安裝指令」\n",
    "# python-Levenshtein 是一個輔助工具，能讓 fuzzywuzzy 跑得快 10 倍\n",
    "!pip install fuzzywuzzy python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da13d6ce-597d-430a-895e-2ac2b98ca30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始資料: 2732 筆\n",
      "移除 177 筆空白標題後，剩下: 2555 筆文獻可供比對。\n",
      "\n",
      "--- 資料已清理乾淨，並轉換為小寫 ---\n",
      "   Year                                              Title  \\\n",
      "0  2014     32nd AIAA Applied Aerodynamics Conference 2014   \n",
      "1  2018                       Nonlinear Photonics, NP 2018   \n",
      "2  2021  Erratum: Neutron Spin Echo Spectroscopy as a U...   \n",
      "3  2024  11th International Conference on E-Health and ...   \n",
      "4  2024  2024 International Symposium on 3D Analysis of...   \n",
      "\n",
      "                                         Title_lower  \n",
      "0     32nd aiaa applied aerodynamics conference 2014  \n",
      "1                       nonlinear photonics, np 2018  \n",
      "2  erratum: neutron spin echo spectroscopy as a u...  \n",
      "3  11th international conference on e-health and ...  \n",
      "4  2024 international symposium on 3d analysis of...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_21424\\569680722.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['Title_lower'] = df_cleaned['Title'].astype(str).str.lower()\n"
     ]
    }
   ],
   "source": [
    "# 載入我們剛安裝的工具\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# --- 1. 清理資料：移除 177 筆空白標題 ---\n",
    "print(f\"原始資料: {len(clean_df)} 筆\")\n",
    "\n",
    "# .dropna() 就是「丟掉 (drop) 空白值 (na)」\n",
    "# subset=['Title'] 告訴它只檢查 'Title' 欄位\n",
    "df_cleaned = clean_df.dropna(subset=['Title'])\n",
    "\n",
    "print(f\"移除 {177} 筆空白標題後，剩下: {len(df_cleaned)} 筆文獻可供比對。\")\n",
    "\n",
    "\n",
    "# --- 2. 準備資料：全部轉成「小寫」---\n",
    "# 這樣 \"The effect...\" 和 \"the effect...\" 才會被視為相同\n",
    "# 我們建立一個新欄位 'Title_lower' 來存放\n",
    "df_cleaned['Title_lower'] = df_cleaned['Title'].astype(str).str.lower()\n",
    "\n",
    "print(\"\\n--- 資料已清理乾淨，並轉換為小寫 ---\")\n",
    "print(df_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69c75564-a226-4de4-ab42-61e75c102558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 開始搜索「灰色地帶」(相似度 85% - 94%) ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_cleaned' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m UPPER_THRESHOLD \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m94\u001b[39m  \u001b[38;5;66;03m# 我們只抓 94 (含) 以下的\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 2. 為了加快速度，我們重新從 df_cleaned 建立清單\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# (這一步是為了確保 records 變數一定存在)\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m records \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(df_cleaned\u001b[38;5;241m.\u001b[39mitertuples())\n\u001b[0;32m     13\u001b[0m total_records \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(records)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# 3. 準備一個「新的」清單來儲存\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_cleaned' is not defined"
     ]
    }
   ],
   "source": [
    "# --- 我們假設 df_cleaned 和 records 變數都還在 ---\n",
    "# (如果報錯 NameError，我們再用「一鍵恢復」即可)\n",
    "\n",
    "print(\"--- 開始搜索「灰色地帶」(相似度 85% - 94%) ---\")\n",
    "\n",
    "# 1. 設定「新的」門檻\n",
    "LOWER_THRESHOLD = 85\n",
    "UPPER_THRESHOLD = 94  # 我們只抓 94 (含) 以下的\n",
    "\n",
    "# 2. 為了加快速度，我們重新從 df_cleaned 建立清單\n",
    "# (這一步是為了確保 records 變數一定存在)\n",
    "records = list(df_cleaned.itertuples())\n",
    "total_records = len(records)\n",
    "\n",
    "# 3. 準備一個「新的」清單來儲存\n",
    "medium_confidence_duplicates = []\n",
    "\n",
    "# 4. 執行「新的」比對迴圈\n",
    "for i in tqdm(range(total_records), desc=\"正在搜索(85-94%)\"):\n",
    "    for j in range(i + 1, total_records):\n",
    "        \n",
    "        record1 = records[i]\n",
    "        record2 = records[j]\n",
    "        \n",
    "        # 計算分數\n",
    "        score = fuzz.ratio(record1.Title_lower, record2.Title_lower)\n",
    "        \n",
    "        # 【關鍵修改】\n",
    "        # 檢查分數是否「介於」85 和 94 之間\n",
    "        if score >= LOWER_THRESHOLD and score <= UPPER_THRESHOLD:\n",
    "            \n",
    "            # 把這組「可疑」的配對存起來\n",
    "            medium_confidence_duplicates.append((\n",
    "                score,\n",
    "                record1.Year,\n",
    "                record2.Year,\n",
    "                record1.Title,\n",
    "                record2.Title\n",
    "            ))\n",
    "\n",
    "print(f\"\\n--- 灰色地帶搜索完成！ ---\")\n",
    "print(f\"總共找到 {len(medium_confidence_duplicates)} 組『中度相似』 ( {LOWER_THRESHOLD}% ~ {UPPER_THRESHOLD}% ) 的文獻。\")\n",
    "\n",
    "# --- 5. 把結果印出來看看 ---\n",
    "if medium_confidence_duplicates:\n",
    "    # 把它變成一個漂亮的表格\n",
    "    medium_dup_df = pd.DataFrame(medium_confidence_duplicates, columns=[\n",
    "        'Score', 'Year_1', 'Year_2', 'Title_1', 'Title_2'\n",
    "    ])\n",
    "    \n",
    "    # 依照「相似度分數」從高到低排序\n",
    "    medium_dup_df = medium_dup_df.sort_values(by='Score', ascending=False)\n",
    "    \n",
    "    print(\"\\n--- 以下是找到的『中度相似』文獻 (最多顯示 20 組) ---\")\n",
    "    print(medium_dup_df.head(20))\n",
    "    \n",
    "    # 【關鍵】建議您匯出成「另一個」Excel 檔，手動檢查\n",
    "    # 檔名會是 '中度相似文獻.xlsx'\n",
    "    medium_dup_df.to_excel(\"中度相似文獻.xlsx\", index=False)\n",
    "    print(\"\\n已將完整的「中度相似」清單匯出為 '中度相似文獻.xlsx'\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n恭喜！在 {LOWER_THRESHOLD}% ~ {UPPER_THRESHOLD}% 的相似度下，沒有找到可疑的重複文獻。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f9b6c0e-9fb3-416e-bf1e-6f5f059204da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 步驟 1：正在恢復 df (52欄) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_33448\\2648012701.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['Title_lower'] = df_cleaned['Title'].astype(str).str.lower()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 步驟 2：正在恢復 clean_df (年份和標題) ---\n",
      "--- 步驟 3：正在恢復 df_cleaned (移除 177 筆垃圾) ---\n",
      "恢復了 2555 筆文獻可供比對。\n",
      "\n",
      "--- 步驟 4：開始搜索「灰色地帶」(85% - 94%) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "正在搜索(85-94%): 100%|███████████████████████████████████████████████████████████| 2555/2555 [00:23<00:00, 110.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 灰色地帶搜索完成！ ---\n",
      "總共找到 7 組『中度相似』 ( 85% ~ 94% ) 的文獻。\n",
      "\n",
      "--- 以下是找到的『中度相似』文獻 (最多顯示 20 組) ---\n",
      "   Score Year_1 Year_2                                            Title_1  \\\n",
      "3     94   1992   1992  [Rademaker and Garcin syndrome associated with...   \n",
      "5     94   1997   1997  Unilateral laminotomy for bilateral decompress...   \n",
      "0     90   2023   2023  Quadriceps Muscle and Medial Retinaculum Combi...   \n",
      "2     89   2020   2021  [Robotic recovery of walking function in patie...   \n",
      "4     88   1989   1989  [Signs of chronic overheating in miners of dee...   \n",
      "6     88   2005   1999  Harmonic analysis of force platform data in no...   \n",
      "1     86   1991   1992             GAIT CHARACTERISTICS OF OBESE CHILDREN   \n",
      "\n",
      "                                             Title_2  \n",
      "3  RADEMAKER-GARCIN SYNDROME ASSOCIATED WITH PALL...  \n",
      "5  Unilateral laminotomy for bilateral decompress...  \n",
      "0  Quadriceps Muscle and Medial Retinaculum Combi...  \n",
      "2  Robotic Restoration of Gait Function in Patien...  \n",
      "4  Signs of chronic overheating in workers of dee...  \n",
      "6  The variability of force platform data in norm...  \n",
      "1        LOCOMOTOR CHARACTERISTICS OF OBESE CHILDREN  \n",
      "\n",
      "已將完整的「中度相似」清單匯出為 '中度相似文獻.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# --- 【一鍵恢復 + 執行「85-94%」新搜索】---\n",
    "# 請一次執行這整個區塊\n",
    "\n",
    "# --- 1. 載入所有工具 ---\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"--- 步驟 1：正在恢復 df (52欄) ---\")\n",
    "file_path = '系統性文獻回顧.txt'\n",
    "df = pd.read_csv(file_path, sep='\\t', header=None)\n",
    "\n",
    "print(\"--- 步驟 2：正在恢復 clean_df (年份和標題) ---\")\n",
    "clean_df = df[[2, 3]].rename(columns={2: 'Year', 3: 'Title'})\n",
    "\n",
    "print(\"--- 步驟 3：正在恢復 df_cleaned (移除 177 筆垃圾) ---\")\n",
    "df_cleaned = clean_df.dropna(subset=['Title'])\n",
    "df_cleaned['Title_lower'] = df_cleaned['Title'].astype(str).str.lower()\n",
    "print(f\"恢復了 {len(df_cleaned)} 筆文獻可供比對。\")\n",
    "\n",
    "\n",
    "# --- 步驟 4：【新任務】開始搜索「灰色地帶」(相似度 85% - 94%) ---\n",
    "print(\"\\n--- 步驟 4：開始搜索「灰色地帶」(85% - 94%) ---\")\n",
    "\n",
    "LOWER_THRESHOLD = 85\n",
    "UPPER_THRESHOLD = 94\n",
    "\n",
    "records = list(df_cleaned.itertuples())\n",
    "total_records = len(records)\n",
    "medium_confidence_duplicates = []\n",
    "\n",
    "for i in tqdm(range(total_records), desc=\"正在搜索(85-94%)\"):\n",
    "    for j in range(i + 1, total_records):\n",
    "        \n",
    "        record1 = records[i]\n",
    "        record2 = records[j]\n",
    "        \n",
    "        score = fuzz.ratio(record1.Title_lower, record2.Title_lower)\n",
    "        \n",
    "        if score >= LOWER_THRESHOLD and score <= UPPER_THRESHOLD:\n",
    "            medium_confidence_duplicates.append((\n",
    "                score,\n",
    "                record1.Year,\n",
    "                record2.Year,\n",
    "                record1.Title,\n",
    "                record2.Title\n",
    "            ))\n",
    "\n",
    "print(f\"\\n--- 灰色地帶搜索完成！ ---\")\n",
    "print(f\"總共找到 {len(medium_confidence_duplicates)} 組『中度相似』 ( {LOWER_THRESHOLD}% ~ {UPPER_THRESHOLD}% ) 的文獻。\")\n",
    "\n",
    "# --- 步驟 5：顯示結果並匯出 ---\n",
    "if medium_confidence_duplicates:\n",
    "    medium_dup_df = pd.DataFrame(medium_confidence_duplicates, columns=[\n",
    "        'Score', 'Year_1', 'Year_2', 'Title_1', 'Title_2'\n",
    "    ])\n",
    "    medium_dup_df = medium_dup_df.sort_values(by='Score', ascending=False)\n",
    "    \n",
    "    print(\"\\n--- 以下是找到的『中度相似』文獻 (最多顯示 20 組) ---\")\n",
    "    print(medium_dup_df.head(20))\n",
    "    \n",
    "    # 匯出到 Excel 檔\n",
    "    medium_dup_df.to_excel(\"中度相似文獻.xlsx\", index=False)\n",
    "    print(\"\\n已將完整的「中度相似」清單匯出為 '中度相似文獻.xlsx'\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n恭喜！在 {LOWER_THRESHOLD}% ~ {UPPER_THRESHOLD}% 的相似度下，沒有找到可疑的重複文獻。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db0406fc-2418-48ff-82d9-583e6ee3d360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 步驟 1-3：正在恢復 df_cleaned (2555 筆) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_33448\\2263046865.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_cleaned['Title_lower'] = df_cleaned['Title'].astype(str).str.lower()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "恢復了 2555 筆文獻可供比對。\n",
      "\n",
      "--- 步驟 4a：正在執行「高可信度」(>=95%) 比對... ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "比對進度(>=95%): 100%|████████████████████████████████████████████████████████████| 2555/2555 [00:22<00:00, 113.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到 443 篇「高可信度」重複文獻。\n",
      "\n",
      "--- 步驟 4b：正在執行「中度可信度」(85-94%) 比對... ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "比對進度(85-94%): 100%|███████████████████████████████████████████████████████████| 2555/2555 [00:21<00:00, 117.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到 7 組「中度可信」配對 (7 組)。\n",
      "\n",
      "--- 步驟 5：正在套用您的手動決策... ---\n",
      "在 7 組中度配對中，已排除 2 組。準備刪除 5 篇「中度」重複文獻。\n",
      "總計 443 (高) + 5 (中) = 448 篇文獻將被刪除。\n",
      "\n",
      "--- 最終清理完成！ ---\n",
      "原始 2555 筆文獻，減去 448 筆經您確認的重複文獻。\n",
      "您『最終乾淨』的文獻庫剩下：1764 筆\n",
      "   Year                                              Title  \\\n",
      "0  2014     32nd AIAA Applied Aerodynamics Conference 2014   \n",
      "1  2018                       Nonlinear Photonics, NP 2018   \n",
      "2  2021  Erratum: Neutron Spin Echo Spectroscopy as a U...   \n",
      "3  2024  11th International Conference on E-Health and ...   \n",
      "4  2024  2024 International Symposium on 3D Analysis of...   \n",
      "\n",
      "                                         Title_lower  \n",
      "0     32nd aiaa applied aerodynamics conference 2014  \n",
      "1                       nonlinear photonics, np 2018  \n",
      "2  erratum: neutron spin echo spectroscopy as a u...  \n",
      "3  11th international conference on e-health and ...  \n",
      "4  2024 international symposium on 3d analysis of...  \n",
      "\n",
      "已將「最終乾淨」的資料庫匯出為 Excel 檔！\n"
     ]
    }
   ],
   "source": [
    "# --- 【最終清理腳Go 1. 載入所有工具 ---\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 2. 【恢復】讀取與清理 (步驟 1-3) ---\n",
    "print(\"--- 步驟 1-3：正在恢復 df_cleaned (2555 筆) ---\")\n",
    "file_path = '系統性文獻回顧.txt'\n",
    "df = pd.read_csv(file_path, sep='\\t', header=None)\n",
    "clean_df = df[[2, 3]].rename(columns={2: 'Year', 3: 'Title'})\n",
    "df_cleaned = clean_df.dropna(subset=['Title'])\n",
    "df_cleaned['Title_lower'] = df_cleaned['Title'].astype(str).str.lower()\n",
    "print(f\"恢復了 {len(df_cleaned)} 筆文獻可供比對。\")\n",
    "\n",
    "records = list(df_cleaned.itertuples())\n",
    "total_records = len(records)\n",
    "\n",
    "# --- 3. 【高可信度】執行 >=95% 的比對 ---\n",
    "print(\"\\n--- 步驟 4a：正在執行「高可信度」(>=95%) 比對... ---\")\n",
    "threshold_high = 95\n",
    "high_confidence_duplicates = []\n",
    "\n",
    "for i in tqdm(range(total_records), desc=\"比對進度(>=95%)\"):\n",
    "    for j in range(i + 1, total_records):\n",
    "        score = fuzz.ratio(records[i].Title_lower, records[j].Title_lower)\n",
    "        if score >= threshold_high:\n",
    "            high_confidence_duplicates.append(records[j].Title) # 我們只記錄要刪除的 Title_2\n",
    "\n",
    "# 準備「高可信度」的刪除清單\n",
    "high_confidence_titles_to_remove = set(high_confidence_duplicates)\n",
    "print(f\"找到 {len(high_confidence_titles_to_remove)} 篇「高可信度」重複文獻。\") # 這應該是 443\n",
    "\n",
    "# --- 4. 【中度可信度】執行 85-94% 的比對 ---\n",
    "print(\"\\n--- 步驟 4b：正在執行「中度可信度」(85-94%) 比對... ---\")\n",
    "threshold_low = 85\n",
    "threshold_medium = 94\n",
    "medium_confidence_duplicates_raw = [] # 儲存所有 7 組\n",
    "\n",
    "for i in tqdm(range(total_records), desc=\"比對進度(85-94%)\"):\n",
    "    for j in range(i + 1, total_records):\n",
    "        score = fuzz.ratio(records[i].Title_lower, records[j].Title_lower)\n",
    "        if score >= threshold_low and score <= threshold_medium:\n",
    "            # 這次我們儲存完整的配對資訊，方便過濾\n",
    "            medium_confidence_duplicates_raw.append(\n",
    "                (records[i].Title, records[j].Title)\n",
    "            )\n",
    "print(f\"找到 {len(medium_confidence_duplicates_raw)} 組「中度可信」配對 (7 組)。\") # 這應該是 7\n",
    "\n",
    "# --- 5. 【您的決策】套用您的手動過濾 ---\n",
    "print(\"\\n--- 步驟 5：正在套用您的手動決策... ---\")\n",
    "# 這是您「決定不要刪除」的 2 組文獻中的 Title_1\n",
    "titles_to_keep_safe = [\n",
    "    'Harmonic analysis of force platform data in normal and cerebral palsy gait',\n",
    "    'GAIT CHARACTERISTICS OF OBESE CHILDREN'\n",
    "]\n",
    "\n",
    "# 準備「中度可信度」的刪除清單\n",
    "medium_confidence_titles_to_remove = set()\n",
    "for title1, title2 in medium_confidence_duplicates_raw:\n",
    "    # 檢查 Title_1 是不是在您的「安全名單」中\n",
    "    if title1 not in titles_to_keep_safe:\n",
    "        # 如果「不是」，就把 Title_2 加入刪除清單\n",
    "        medium_confidence_titles_to_remove.add(title2)\n",
    "\n",
    "print(f\"在 7 組中度配對中，已排除 2 組。準備刪除 {len(medium_confidence_titles_to_remove)} 篇「中度」重複文獻。\") # 這應該是 5\n",
    "\n",
    "# --- 6. 【合併】合併兩個刪除清單 ---\n",
    "total_titles_to_remove = high_confidence_titles_to_remove.union(medium_confidence_titles_to_remove)\n",
    "print(f\"總計 {len(high_confidence_titles_to_remove)} (高) + {len(medium_confidence_titles_to_remove)} (中) = {len(total_titles_to_remove)} 篇文獻將被刪除。\")\n",
    "\n",
    "# --- 7. 【產出】產生「最終乾淨」資料庫 ---\n",
    "# ~ 符號代表「不是」(NOT)\n",
    "# .isin() 代表「包含在...清單中」\n",
    "df_final_unique = df_cleaned[~df_cleaned['Title'].isin(total_titles_to_remove)]\n",
    "\n",
    "print(f\"\\n--- 最終清理完成！ ---\")\n",
    "print(f\"原始 2555 筆文獻，減去 {len(total_titles_to_remove)} 筆經您確認的重複文獻。\")\n",
    "print(f\"您『最終乾淨』的文獻庫剩下：{len(df_final_unique)} 筆\")\n",
    "print(df_final_unique.head())\n",
    "\n",
    "# 順便匯出「最終乾淨」的資料庫，方便您下次直接讀取\n",
    "df_final_unique.to_excel(\"最終乾淨文獻庫_17xx.xlsx\", index=False)\n",
    "print(\"\\n已將「最終乾淨」的資料庫匯出為 Excel 檔！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a686d15-5559-49ed-9a7d-4c8ec325b568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "# 這一步會安裝一個「進度條」工具 (tqdm)\n",
    "# 讓我們在等待時可以看到進度，才不會以為當機了\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61b00c6b-a3bb-49b1-9060-54149e69e2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "開始比對 2555 筆文獻... 這會需要幾分鐘，請稍候。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "正在比對進度: 100%|████████████████████████████████████████████████████████████████| 2555/2555 [00:26<00:00, 95.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 比對完成！ ---\n",
      "總共找到 536 組『高度相似』 (>= 95%) 的文獻。\n",
      "\n",
      "--- 以下是找到的高度相似文獻 (最多顯示 20 組) ---\n",
      "     Score Year_1 Year_2                                            Title_1  \\\n",
      "0      100   2015   2015  Relationship of plasma level of chemerin and v...   \n",
      "355    100   2025   2025  Analysis of dog movement using a single accele...   \n",
      "353    100   2006   2006  Performance characterization of siemens primus...   \n",
      "352    100   2022   2022  Validity and Reliability of a Smartphone App f...   \n",
      "351    100   2021   2022  Validity and Reliability of a Smartphone App f...   \n",
      "350    100   2021   2022  Validity and Reliability of a Smartphone App f...   \n",
      "349    100   2015   2015  Analysis of static and dynamic balance in heal...   \n",
      "348    100   2024   2024  SmartStride: Quantifying Strides of Individual...   \n",
      "347    100   2023   2023  Total Differential Photometric Mesh Refinement...   \n",
      "346    100   2018   2018  MEMS Inertial Sensors Based Gait Analysis for ...   \n",
      "345    100   2019   2019  Body Sensor Network-Based Gait Quality Assessm...   \n",
      "344    100   2024   2024  Assessing the Stability of Human Gait Based on...   \n",
      "343    100   2017   2017  Anti-N-methyl-D-aspartate receptor(NMDAR) anti...   \n",
      "342    100   2019   2019  Nanostructured porous CrN thin films by obliqu...   \n",
      "341    100   2025   2025  A Task-Agnostic Approach to Unified Multi-Acti...   \n",
      "340    100   2025   2025  A Task-Agnostic Approach to Unified Multi-Acti...   \n",
      "339    100   2025   2025  A Task-Agnostic Approach to Unified Multi-Acti...   \n",
      "337    100   2020   2020  Presurgical identification of primary central ...   \n",
      "336    100   2011   2011  Postural instability and fall risk in Parkinso...   \n",
      "335    100   2020   2020  Comparison of visual lameness scores to gait a...   \n",
      "\n",
      "                                               Title_2  \n",
      "0    Relationship of plasma level of chemerin and v...  \n",
      "355  Analysis of dog movement using a single accele...  \n",
      "353  Performance characterization of Siemens Primus...  \n",
      "352  Validity and reliability of a smartphone app f...  \n",
      "351  Validity and reliability of a smartphone app f...  \n",
      "350  Validity and Reliability of a Smartphone App f...  \n",
      "349  Analysis of static and dynamic balance in heal...  \n",
      "348  SmartStride: Quantifying Strides of Individual...  \n",
      "347  Total Differential Photometric Mesh Refinement...  \n",
      "346  MEMS Inertial Sensors Based Gait Analysis for ...  \n",
      "345  Body Sensor Network-Based Gait Quality Assessm...  \n",
      "344  Assessing the Stability of Human Gait Based on...  \n",
      "343  Anti-N-methyl-D-aspartate receptor(NMDAR) anti...  \n",
      "342  Nanostructured porous CrN thin films by obliqu...  \n",
      "341  A Task-Agnostic Approach to Unified Multi-Acti...  \n",
      "340  A Task-Agnostic Approach to Unified Multi-Acti...  \n",
      "339  A Task-Agnostic Approach to Unified Multi-Acti...  \n",
      "337  Presurgical Identification of Primary Central ...  \n",
      "336  Postural instability and fall risk in Parkinso...  \n",
      "335  Comparison of visual lameness scores to gait a...  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 載入「進度條」和「模糊比對」工具\n",
    "from tqdm import tqdm\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# 我們要儲存結果的地方\n",
    "duplicates_to_check = []\n",
    "\n",
    "# 設定一個「相似度」門檻\n",
    "# 95 代表兩篇文章標題的相似度高達 95%\n",
    "threshold = 95\n",
    "\n",
    "# 為了加快速度，我們先把資料轉成一個清單\n",
    "records = list(df_cleaned.itertuples())\n",
    "total_records = len(records)\n",
    "\n",
    "print(f\"開始比對 {total_records} 筆文獻... 這會需要幾分鐘，請稍候。\")\n",
    "\n",
    "# --- 這是核心的比對迴圈 ---\n",
    "# tqdm(...) 會幫我們畫出進度條\n",
    "for i in tqdm(range(total_records), desc=\"正在比對進度\"):\n",
    "    # 第二層迴圈，j 永遠從 i 的下一個開始\n",
    "    # (這樣 A 和 B 比過，B 和 A 就不會再比一次)\n",
    "    for j in range(i + 1, total_records):\n",
    "        \n",
    "        # 從清單中取出兩筆紀錄\n",
    "        record1 = records[i]\n",
    "        record2 = records[j]\n",
    "        \n",
    "        # 計算兩筆「小寫標題」的相似度分數\n",
    "        score = fuzz.ratio(record1.Title_lower, record2.Title_lower)\n",
    "        \n",
    "        # 如果分數高於我們的門檻 (95)\n",
    "        if score >= threshold:\n",
    "            # 就把這兩筆的「原始標題」和「分數」存起來\n",
    "            duplicates_to_check.append((\n",
    "                score,\n",
    "                record1.Year,\n",
    "                record2.Year,\n",
    "                record1.Title,\n",
    "                record2.Title\n",
    "            ))\n",
    "\n",
    "print(f\"\\n--- 比對完成！ ---\")\n",
    "print(f\"總共找到 {len(duplicates_to_check)} 組『高度相似』 (>= {threshold}%) 的文獻。\")\n",
    "\n",
    "# --- 最後，把結果印出來看看 ---\n",
    "if duplicates_to_check:\n",
    "    # 把它變成一個漂亮的表格\n",
    "    dup_df = pd.DataFrame(duplicates_to_check, columns=[\n",
    "        'Score', 'Year_1', 'Year_2', 'Title_1', 'Title_2'\n",
    "    ])\n",
    "    \n",
    "    # 依照「相似度分數」從高到低排序\n",
    "    dup_df = dup_df.sort_values(by='Score', ascending=False)\n",
    "    \n",
    "    print(\"\\n--- 以下是找到的高度相似文獻 (最多顯示 20 組) ---\")\n",
    "    print(dup_df.head(20))\n",
    "    \n",
    "    # 【關鍵】您可以把這個結果匯出成 Excel 檔！\n",
    "    # 檔名會是 '重複文獻清單.xlsx'\n",
    "    # dup_df.to_excel(\"重複文獻清單.xlsx\", index=False)\n",
    "    # print(\"\\n已將完整的重複清單匯出為 '重複文獻清單.xlsx'\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n恭喜！在 {threshold}% 的相似度下，沒有找到明顯的重複文獻。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbc0ba18-6223-41c6-8519-482fd488f4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 成功！已將 536 組重複清單匯出為 '重複文獻清單.xlsx' ---\n"
     ]
    }
   ],
   "source": [
    "# --- 任務 1：將 536 組重複清單匯出為 Excel 檔 ---\n",
    "# 檔名會是 '重複文獻清單.xlsx'\n",
    "# 這樣您就可以在 Excel 中手動檢視這 536 組\n",
    "dup_df.to_excel(\"重複文獻清單.xlsx\", index=False)\n",
    "\n",
    "print(f\"--- 成功！已將 {len(dup_df)} 組重複清單匯出為 '重複文獻清單.xlsx' ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f290803e-c28a-42dd-9fbe-60eb58167a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在 536 組重複配對中，共找到 443 個不重複的『Title_2』標題。\n",
      "原始 2555 筆文獻，減去 443 筆被標記的重複文獻。\n",
      "--- 您『最終乾淨』的文獻庫剩下：1769 筆 ---\n",
      "\n",
      "這是您『最終乾淨』資料庫的前 5 筆：\n",
      "   Year                                              Title  \\\n",
      "0  2014     32nd AIAA Applied Aerodynamics Conference 2014   \n",
      "1  2018                       Nonlinear Photonics, NP 2018   \n",
      "2  2021  Erratum: Neutron Spin Echo Spectroscopy as a U...   \n",
      "3  2024  11th International Conference on E-Health and ...   \n",
      "4  2024  2024 International Symposium on 3D Analysis of...   \n",
      "\n",
      "                                         Title_lower  \n",
      "0     32nd aiaa applied aerodynamics conference 2014  \n",
      "1                       nonlinear photonics, np 2018  \n",
      "2  erratum: neutron spin echo spectroscopy as a u...  \n",
      "3  11th international conference on e-health and ...  \n",
      "4  2024 international symposium on 3d analysis of...  \n"
     ]
    }
   ],
   "source": [
    "# --- 任務 2：建立「最終乾淨」的資料庫 ---\n",
    "\n",
    "# 1. 取得所有「被視為重複」的標題 (Title_2)\n",
    "# 我們用 set() 來確保清單中的標題是唯一的\n",
    "# (因為 A=B, A=C, 這樣 B 和 C 都會被移除)\n",
    "titles_to_remove = set(dup_df['Title_2'])\n",
    "\n",
    "print(f\"在 {len(dup_df)} 組重複配對中，共找到 {len(titles_to_remove)} 個不重複的『Title_2』標題。\")\n",
    "\n",
    "# 2. 從我們原本的 2555 筆資料中，過濾掉這些標題\n",
    "# ~ 符號代表「不是」(NOT)\n",
    "# .isin() 代表「包含在...清單中」\n",
    "# ~df_cleaned['Title'].isin(titles_to_remove) 就是「標題*沒有*在移除清單中」\n",
    "df_final_unique = df_cleaned[~df_cleaned['Title'].isin(titles_to_remove)]\n",
    "\n",
    "print(f\"原始 2555 筆文獻，減去 {len(titles_to_remove)} 筆被標記的重複文獻。\")\n",
    "print(f\"--- 您『最終乾淨』的文獻庫剩下：{len(df_final_unique)} 筆 ---\")\n",
    "\n",
    "print(\"\\n這是您『最終乾淨』資料庫的前 5 筆：\")\n",
    "print(df_final_unique.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46387ea-6536-4395-b9c0-35b2b068163b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
